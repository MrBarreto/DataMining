{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from config import load_config\n",
    "import pandas as pd\n",
    "import folium\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capturar_data_mapa(conn, table_name, linha):\n",
    "    cur = conn.cursor()\n",
    "    all_data = [] \n",
    "    try:\n",
    "        fetch_query = f'''\n",
    "        SELECT latitude::double precision, longitude::double precision\n",
    "        FROM {table_name}\n",
    "        WHERE linha='{linha}'\n",
    "        '''\n",
    "        cur.execute(fetch_query)\n",
    "        rows = cur.fetchall()\n",
    "        all_data.extend(rows)\n",
    "    except Exception as e:\n",
    "            print(f\"Erro ao executar a query na tabela {table_name}: {e}\")\n",
    "            conn.rollback()\n",
    "    else:\n",
    "         conn.commit()\n",
    "    cur.close()\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_dados_tabela(table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # Adicionar a coluna hour, se não existir\n",
    "        cur.execute(f\"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS hour INTEGER;\")\n",
    "        \n",
    "        # Atualizar a coluna hour com base no datahora\n",
    "        cur.execute(f\"UPDATE {table_name} SET hour = EXTRACT(HOUR FROM TO_TIMESTAMP(datahora / 1000));\")\n",
    "        \n",
    "        # Criar uma nova tabela filtrada\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE TABLE {table_name}_filter AS\n",
    "            SELECT *\n",
    "            FROM {table_name}\n",
    "            WHERE hour BETWEEN 8 AND 22;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Remover a coluna hour das tabelas\n",
    "        cur.execute(f\"ALTER TABLE {table_name} DROP COLUMN hour;\")\n",
    "        cur.execute(f\"ALTER TABLE {table_name}_filter DROP COLUMN hour;\")\n",
    "        \n",
    "        # Commit das alterações\n",
    "        conn.commit()\n",
    "        print(f\"Alterações na tabela {table_name} foram comitadas com sucesso.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Erro ao limpar dados da tabela {table_name}: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        print(f\"Cursor fechado para a tabela {table_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adicionar_geom(table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # Adicionar a coluna geom se não existir\n",
    "        cur.execute(f\"\"\"\n",
    "            ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS geom geography(Point, 4326);\n",
    "        \"\"\")\n",
    "        \n",
    "        # Atualizar a coluna geom com os valores de longitude e latitude\n",
    "        cur.execute(f\"\"\"\n",
    "            UPDATE {table_name}\n",
    "            SET geom = ST_SetSRID(ST_MakePoint(longitude::double precision, latitude::double precision), 4326);\n",
    "        \"\"\")\n",
    "        \n",
    "        # Commit das alterações\n",
    "        conn.commit()\n",
    "        print(f\"Coluna geom criada e preenchida na tabela {table_name}_filter com sucesso.\")\n",
    "    except Exception as e:\n",
    "        # Rollback em caso de erro\n",
    "        conn.rollback()\n",
    "        print(f\"Erro ao atualizar a tabela {table_name}_filter: {e}\")\n",
    "    finally:\n",
    "        # Fechar o cursor\n",
    "        cur.close()\n",
    "        print(f\"Cursor fechado para a tabela {table_name}_filter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtra_linhas(table_name, linhas_de_interesse, conn):\n",
    "    cur = conn.cursor()\n",
    "    temp_table_name = table_name + '_intermediate'\n",
    "    linhas_str = ', '.join([f\"'{linha}'\" for linha in linhas_de_interesse])\n",
    "    try:\n",
    "        # Criar tabela intermediária filtrada\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE TABLE {temp_table_name} AS\n",
    "            SELECT *\n",
    "            FROM {table_name}_filter\n",
    "            WHERE linha IN ({linhas_str});\n",
    "        \"\"\")\n",
    "        print(f\"Tabela intermediária {temp_table_name} criada com sucesso.\")\n",
    "        conn.commit()\n",
    "        print(f\"Alterações na tabela {temp_table_name} foram comitadas com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar a tabela intermediária {temp_table_name}: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "        print(f\"Cursor fechado para a tabela {temp_table_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobrescreve_tabelas(table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    temp_table_name = table_name + '_intermediate'\n",
    "    try:\n",
    "        # Verificar se a tabela temporária existe\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT EXISTS (\n",
    "                SELECT FROM pg_tables\n",
    "                WHERE schemaname = 'public' AND tablename = '{temp_table_name}'\n",
    "            );\n",
    "        \"\"\")\n",
    "        exists = cur.fetchone()[0]\n",
    "\n",
    "        if exists:\n",
    "            # Excluir a tabela original\n",
    "            cur.execute(f\"DROP TABLE IF EXISTS {table_name}_filter;\")\n",
    "            # Renomear a tabela temporária para o nome original\n",
    "            cur.execute(f\"ALTER TABLE {temp_table_name} RENAME TO {table_name}_filter;\")\n",
    "            print(f\"Tabela {table_name}_filter sobrescrita com sucesso.\")\n",
    "            conn.commit()\n",
    "            print(f\"Tabela {table_name}_filter comitada com sucesso.\")\n",
    "        else:\n",
    "            print(f\"Tabela temporária {temp_table_name} não encontrada.\")\n",
    "            conn.rollback()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao sobrescrever a tabela {table_name}_filter: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "        print(f\"Cursor fechado para a tabela {table_name}_filter.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margem_erro(coord1, coord2, radius=100):\n",
    "    return geodesic(coord1, coord2).meters <= radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_tabela_destino(conn):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS combined_table_all AS\n",
    "            SELECT * FROM dia_0105 WHERE 1=0;\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        print(\"Tabela de destino 'combined_table' criada com sucesso.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Erro ao criar a tabela de destino: {e}\")\n",
    "    finally:\n",
    "        cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserir_dados(table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(f\"INSERT INTO combined_table_all SELECT * FROM {table_name};\")\n",
    "        conn.commit()\n",
    "        print(f\"Dados inseridos na tabela 'combined_table' a partir de '{table_name}' com sucesso.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Erro ao inserir dados da tabela {table_name}: {e}\")\n",
    "    finally:\n",
    "        cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(config):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    try:\n",
    "        # connecting to the PostgreSQL server\n",
    "        with psycopg2.connect(**config) as conn:\n",
    "            print('Connected to the PostgreSQL server.')\n",
    "            return conn\n",
    "    except (psycopg2.DatabaseError, Exception) as error:\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the PostgreSQL server.\n"
     ]
    }
   ],
   "source": [
    "config = load_config()\n",
    "conn = connect(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_de_interesse = ['483', '864', '639', '3', '309', '774', '629', '371', '397', '100', '838', '315', '624', '388', '918', '665', '328', '497', '878', '355', '138', '606', '457', '550', '803', '917', '638', '2336', '399', '298', '867', '553', '565', '422', '756', '186012003', '292', '554', '634', '232', '415', '2803', '324', '852', '557', '759', '343', '779', '905', '108']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = ['dia_0105', 'dia_0205', 'dia_0305', 'dia_0405', 'dia_0505', 'dia_0605', 'dia_0705', 'dia_0805', 'dia_0905', 'dia_1005', 'dia_2504', 'dia_2604', 'dia_2704', 'dia_2804', 'dia_2904', 'dia_3004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for table in table_names:\\n    limpar_dados_tabela(table, conn)'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for table in table_names:\n",
    "    limpar_dados_tabela(table, conn)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for table in table_names:\n",
    "    adicionar_geom(table, conn)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for table in table_names:\n",
    "    filtra_linhas(table, linhas_de_interesse, conn)\n",
    "    sobrescreve_tabelas(table, conn)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for linha in linhas_de_interesse:\\n    data = capturar_data_mapa(conn, \\'dia_2704_filter\\', linha)\\n    if data:\\n        avg_latitude = sum([d[0] for d in data]) / len(data)\\n        avg_longitude = sum([d[1] for d in data]) / len(data)\\n        m = folium.Map(location=[avg_latitude, avg_longitude], zoom_start=12)\\n        # Adicione os pontos ao mapa\\n        for lat, lon in data:\\n            folium.CircleMarker(location=[lat, lon], radius=1, color=\\'blue\\').add_to(m)\\n        output_path = os.path.join(f\\'trajetos/output_map_{linha}.html\\')\\n        m.save(output_path)\\n        print(f\"Mapa salvo em {output_path}\")'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for linha in linhas_de_interesse:\n",
    "    data = capturar_data_mapa(conn, 'dia_2704_filter', linha)\n",
    "    if data:\n",
    "        avg_latitude = sum([d[0] for d in data]) / len(data)\n",
    "        avg_longitude = sum([d[1] for d in data]) / len(data)\n",
    "        m = folium.Map(location=[avg_latitude, avg_longitude], zoom_start=12)\n",
    "        # Adicione os pontos ao mapa\n",
    "        for lat, lon in data:\n",
    "            folium.CircleMarker(location=[lat, lon], radius=1, color='blue').add_to(m)\n",
    "        output_path = os.path.join(f'trajetos/output_map_{linha}.html')\n",
    "        m.save(output_path)\n",
    "        print(f\"Mapa salvo em {output_path}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"criar_tabela_destino(conn)\n",
    "for table in table_names:\n",
    "    inserir_dados(table, conn)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_de_parada = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH ordered_points AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        TO_TIMESTAMP(datahora / 1000) AS datahora_ts,\n",
    "        LAG(TO_TIMESTAMP(datahora / 1000)) OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS prev_datahora_ts,\n",
    "        LAG(geom) OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS prev_geom,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS rn\n",
    "    FROM combined_table\n",
    "),\n",
    "same_position_periods AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        datahora_ts,\n",
    "        prev_datahora_ts,\n",
    "        EXTRACT(EPOCH FROM (datahora_ts - prev_datahora_ts)) AS duration,\n",
    "        CASE \n",
    "            WHEN ST_DWithin(geom, prev_geom, 15) THEN 1\n",
    "            ELSE 0\n",
    "        END AS is_stationary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem ORDER BY datahora_ts) - \n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem, CASE WHEN ST_DWithin(geom, prev_geom, 15) THEN 1 ELSE 0 END ORDER BY datahora_ts) AS grp\n",
    "    FROM ordered_points\n",
    "),\n",
    "stationary_groups AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        datahora_ts,\n",
    "        prev_datahora_ts,\n",
    "        duration,\n",
    "        is_stationary,\n",
    "        grp\n",
    "    FROM same_position_periods\n",
    "    WHERE is_stationary = 1\n",
    ")\n",
    "SELECT\n",
    "    ordem,\n",
    "    linha,\n",
    "    MIN(prev_datahora_ts) AS start_time,\n",
    "    MAX(datahora_ts) AS end_time,\n",
    "    SUM(duration) AS total_duration,\n",
    "    ST_X(geom::geometry) AS longitude,\n",
    "    ST_Y(geom::geometry) AS latitude,\n",
    "    grp\n",
    "FROM stationary_groups\n",
    "GROUP BY ordem, linha, grp, geom\n",
    "HAVING SUM(duration) >= 600; -- 10 minutes in seconds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7403/1168034203.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates(subset=['grp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in linhas_de_interesse:\n",
    "    pontos_de_parada[linha]=[]\n",
    "    for _, row in df_unique.loc[df_unique['linha'] == linha].iterrows():\n",
    "        lat, lon = row['latitude'], row['longitude']\n",
    "        ponto = (lat, lon)\n",
    "        if margem_erro(ponto, pontos_garagem[linha]):\n",
    "             continue\n",
    "        encontrado = False\n",
    "        if row['total_duration'] > 2400:\n",
    "            continue\n",
    "        for pontos in pontos_de_parada[linha]:\n",
    "            if margem_erro(pontos[0], ponto):\n",
    "                encontrado = True\n",
    "                pontos[1] += 1\n",
    "                break\n",
    "        if not encontrado:\n",
    "            pontos_de_parada[linha].append([ponto, 1])\n",
    "for linha in pontos_de_parada.keys():\n",
    "        pontos_de_parada[linha] = sorted(pontos_de_parada[linha], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_finais = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in linhas_de_interesse:\n",
    "    print(f\"{linha} {pontos_de_parada[linha][:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "397\n",
    "388\n",
    "759\n",
    "138 \n",
    "tiveram resultados inconclusivos, adicionarei manualmente os pontos finais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in linhas_de_interesse:\n",
    "    pontos_de_parada_linha = pontos_de_parada[linha]\n",
    "    if len(pontos_de_parada[linha]) > 0:\n",
    "        if (pontos_de_parada_linha[1][1] - pontos_de_parada_linha[2][1]) < 0.2*pontos_de_parada_linha[2][1]:\n",
    "            d2 = geodesic(pontos_de_parada_linha[0][0], pontos_de_parada_linha[1][0]).meters\n",
    "            d3 = geodesic(pontos_de_parada_linha[0][0], pontos_de_parada_linha[2][0]).meters\n",
    "            if d2 > d3:\n",
    "                pontos_finais[linha] = [pontos_de_parada_linha[0][0], pontos_de_parada_linha[1][0]]\n",
    "            else:\n",
    "                pontos_finais[linha] = [pontos_de_parada_linha[0][0], pontos_de_parada_linha[2][0]]\n",
    "        else:\n",
    "            pontos_finais[linha] = [pontos_de_parada_linha[0][0], pontos_de_parada_linha[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in pontos_finais.keys():\n",
    "    print(f\"{linha} {pontos_finais[linha]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_finais['397'] = [(-22.90202, -43.55532), (-22.90121, -43.17778)]\n",
    "pontos_finais['388'] = [(-22.93554, -43.65626), (-22.90121, -43.17778)]\n",
    "pontos_finais['759'] = [(-22.93554, -43.65626), (-22.83152, -43.34393)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criar_tabela_destino(conn)\n",
    "for table in table_names:\n",
    "    inserir_dados(table, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_garagem = \"\"\"\n",
    "WITH ordered_points AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        TO_TIMESTAMP(datahora / 1000) AS datahora_ts,\n",
    "        LAG(TO_TIMESTAMP(datahora / 1000)) OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS prev_datahora_ts,\n",
    "        LAG(geom) OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS prev_geom,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS rn\n",
    "    FROM combined_table_all\n",
    "),\n",
    "same_position_periods AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        datahora_ts,\n",
    "        prev_datahora_ts,\n",
    "        EXTRACT(EPOCH FROM (datahora_ts - prev_datahora_ts)) AS duration,\n",
    "        CASE \n",
    "            WHEN ST_DWithin(geom, prev_geom, 15) THEN 1\n",
    "            ELSE 0\n",
    "        END AS is_stationary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem ORDER BY datahora_ts) - \n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem, CASE WHEN ST_DWithin(geom, prev_geom, 15) THEN 1 ELSE 0 END ORDER BY datahora_ts) AS grp\n",
    "    FROM ordered_points\n",
    "),\n",
    "stationary_groups AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        datahora_ts,\n",
    "        prev_datahora_ts,\n",
    "        duration,\n",
    "        is_stationary,\n",
    "        grp\n",
    "    FROM same_position_periods\n",
    "    WHERE is_stationary = 1\n",
    ")\n",
    "SELECT\n",
    "    ordem,\n",
    "    linha,\n",
    "    MIN(prev_datahora_ts) AS start_time,\n",
    "    MAX(datahora_ts) AS end_time,\n",
    "    SUM(duration) AS total_duration,\n",
    "    ST_X(geom::geometry) AS longitude,\n",
    "    ST_Y(geom::geometry) AS latitude,\n",
    "    grp\n",
    "FROM stationary_groups\n",
    "WHERE EXTRACT(HOUR FROM datahora_ts) >= 22 OR EXTRACT(HOUR FROM datahora_ts) < 5\n",
    "GROUP BY ordem, linha, grp, geom\n",
    "HAVING SUM(duration) >= 10800; -- 10 minutes in seconds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7675/3362709450.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_garagem = pd.read_sql(query_garagem, conn)\n"
     ]
    }
   ],
   "source": [
    "df_garagem = pd.read_sql(query_garagem, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_garagem = df_garagem.drop_duplicates(subset=['grp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_de_parada = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in linhas_de_interesse:\n",
    "    pontos_de_parada[linha]=[]\n",
    "    for _, row in df_garagem.loc[df_garagem['linha'] == linha].iterrows():\n",
    "        lat, lon = row['latitude'], row['longitude']\n",
    "        ponto = (lat, lon)\n",
    "        encontrado = False\n",
    "        for pontos in pontos_de_parada[linha]:\n",
    "            if margem_erro(pontos[0], ponto, radius=300):\n",
    "                encontrado = True\n",
    "                pontos[1] += 1\n",
    "                break\n",
    "        if not encontrado:\n",
    "            pontos_de_parada[linha].append([ponto, 1])\n",
    "for linha in pontos_de_parada.keys():\n",
    "        pontos_de_parada[linha] = sorted(pontos_de_parada[linha], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in pontos_de_parada.keys():\n",
    "    print(f\"{linha} {pontos_de_parada[linha]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "759, 388 não apresentaram resultados conclusivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_garagem = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in linhas_de_interesse:\n",
    "    pontos_de_garagem_linha = pontos_de_parada[linha]\n",
    "    if len(pontos_de_parada[linha]) > 0:\n",
    "        pontos_garagem[linha] = pontos_de_garagem_linha[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in pontos_garagem.keys():\n",
    "    print(f\"{linha} {pontos_garagem[linha]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_garagem = \"\"\"\n",
    "WITH ordered_points AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        TO_TIMESTAMP(datahora / 1000) AS datahora_ts,\n",
    "        LAG(TO_TIMESTAMP(datahora / 1000)) OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS prev_datahora_ts,\n",
    "        LAG(geom) OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS prev_geom,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem ORDER BY TO_TIMESTAMP(datahora / 1000)) AS rn\n",
    "    FROM combined_table\n",
    "),\n",
    "same_position_periods AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        datahora_ts,\n",
    "        prev_datahora_ts,\n",
    "        EXTRACT(EPOCH FROM (datahora_ts - prev_datahora_ts)) AS duration,\n",
    "        CASE \n",
    "            WHEN ST_DWithin(geom, prev_geom, 15) THEN 1\n",
    "            ELSE 0\n",
    "        END AS is_stationary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem ORDER BY datahora_ts) - \n",
    "        ROW_NUMBER() OVER (PARTITION BY ordem, CASE WHEN ST_DWithin(geom, prev_geom, 15) THEN 1 ELSE 0 END ORDER BY datahora_ts) AS grp\n",
    "    FROM ordered_points\n",
    "),\n",
    "stationary_groups AS (\n",
    "    SELECT\n",
    "        ordem,\n",
    "        linha,\n",
    "        geom,\n",
    "        datahora_ts,\n",
    "        prev_datahora_ts,\n",
    "        duration,\n",
    "        is_stationary,\n",
    "        grp\n",
    "    FROM same_position_periods\n",
    "    WHERE is_stationary = 1\n",
    ")\n",
    "SELECT\n",
    "    ordem,\n",
    "    linha,\n",
    "    MIN(prev_datahora_ts) AS start_time,\n",
    "    MAX(datahora_ts) AS end_time,\n",
    "    SUM(duration) AS total_duration,\n",
    "    ST_X(geom::geometry) AS longitude,\n",
    "    ST_Y(geom::geometry) AS latitude,\n",
    "    grp\n",
    "FROM stationary_groups\n",
    "GROUP BY ordem, linha, grp, geom\n",
    "HAVING SUM(duration) >= 10800; -- 10 minutes in seconds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_garagem = pd.read_sql(query_garagem, conn)\n",
    "df_garagem = df_garagem.drop_duplicates(subset=['grp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_de_parada = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in linhas_de_interesse:\n",
    "    pontos_de_parada[linha]=[]\n",
    "    for _, row in df_garagem.loc[df_garagem['linha'] == linha].iterrows():\n",
    "        lat, lon = row['latitude'], row['longitude']\n",
    "        ponto = (lat, lon)\n",
    "        encontrado = False\n",
    "        for pontos in pontos_de_parada[linha]:\n",
    "            if margem_erro(pontos[0], ponto, radius=300):\n",
    "                encontrado = True\n",
    "                pontos[1] += 1\n",
    "                break\n",
    "        if not encontrado:\n",
    "            pontos_de_parada[linha].append([ponto, 1])\n",
    "for linha in pontos_de_parada.keys():\n",
    "        pontos_de_parada[linha] = sorted(pontos_de_parada[linha], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in pontos_de_parada.keys():\n",
    "    print(f\"{linha} {pontos_de_parada[linha]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_garagem['759'] = (-22.93569, -43.65591)\n",
    "pontos_garagem['388'] = (-22.93543, -43.65633)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo = 'dicionarios.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(arquivo, 'w') as f:\n",
    "    json.dump({'dicionario1': pontos_finais, 'dicionario2': pontos_garagem}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(arquivo, 'r') as f:\n",
    "    dados = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos_finais = dados['dicionario1']\n",
    "pontos_garagem = dados['dicionario2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4931/3054511583.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  fatia = pd.read_sql(\"SELECT * FROM combined_table WHERE linha = '315'\", conn)\n"
     ]
    }
   ],
   "source": [
    "fatia = pd.read_sql(\"SELECT * FROM combined_table WHERE linha = '315'\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fatia.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'ordem', 'latitude', 'longitude', 'datahora', 'velocidade',\n",
       "       'linha', 'datahoraenvio', 'datahoraservidor', 'geom', 'unique_id',\n",
       "       'sentido'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fatia.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatia['sentido'] = -1\n",
    "ordens = fatia['ordem'].unique()\n",
    "ultimo_final = ()\n",
    "list_temp = []\n",
    "to_delete = []\n",
    "garagem_ultimo = 0\n",
    "for ordem in ordens:\n",
    "    sentido = 0\n",
    "    filtrados = fatia[fatia['ordem'] == ordem].sort_values(by='datahora').copy()\n",
    "    for index, row in filtrados.iterrows():\n",
    "        fatia.loc[fatia['unique_id'] == row['unique_id'], 'sentido'] = sentido\n",
    "        ponto = (row['latitude'], row['longitude'])\n",
    "        list_temp.append(row['unique_id'])\n",
    "        if margem_erro(ponto, pontos_finais[row['linha']][0]) and pontos_finais[row['linha']][0] != ultimo_final:\n",
    "            if garagem_ultimo == 1:\n",
    "                garagem_ultimo = 0\n",
    "                to_delete.append(list_temp[:])\n",
    "            sentido = abs(sentido -1)\n",
    "            list_temp[:] = []\n",
    "            ultimo_final = pontos_finais[row['linha']][0]\n",
    "        elif margem_erro(ponto, pontos_finais[row['linha']][1]) and pontos_finais[row['linha']][1] != ultimo_final:\n",
    "            if garagem_ultimo == 1:\n",
    "                garagem_ultimo = 0\n",
    "                to_delete.append(list_temp[:])\n",
    "            sentido = abs(sentido -1)\n",
    "            list_temp = []\n",
    "            ultimo_final = pontos_finais[row['linha']][1]\n",
    "        elif margem_erro(ponto, pontos_garagem[row['linha']], radius= 200):\n",
    "            garagem_ultimo = 1\n",
    "        if index == filtrados.index[-1] and garagem_ultimo == 1:\n",
    "            to_delete.append(list_temp[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete_flat = [item for sublist in to_delete for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711461\n"
     ]
    }
   ],
   "source": [
    "print(len(to_delete_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatia = fatia[~fatia['unique_id'].isin(to_delete_flat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentido\n",
       "1    29917\n",
       "0    22893\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fatia['sentido'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql+psycopg2://savio:190876@localhost:5432/t3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_de_interesse = ['483', '864', '639', '3', '309', '774', '629', '371', '100', '838', '315', '624', '388', '918', '665', '328', '497', '878', '355', '606', '457', '550', '803', '917', '638', '2336', '399', '298', '867', '553', '565', '422', '756', '292', '554', '634', '232', '415', '2803', '852', '557', '759', '343', '779', '905', '108']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in corrigir:\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(f\"\"\"\n",
    "            \"DELETE FROM combined_table WHERE linha = '{linha}'\"\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "    print(f\"Processando linha {linha}\")\n",
    "    fatia = pd.read_sql(f\"SELECT * FROM combined_table WHERE linha = '{linha}'\", conn)\n",
    "    fatia['sentido'] = -1\n",
    "    ordens = fatia['ordem'].unique()\n",
    "    ultimo_final = ()\n",
    "    list_temp = []\n",
    "    to_delete = []\n",
    "    garagem_ultimo = 0\n",
    "    for ordem in ordens:\n",
    "        sentido = 0\n",
    "        filtrados = fatia[fatia['ordem'] == ordem].sort_values(by='datahora').copy()\n",
    "        for index, row in filtrados.iterrows():\n",
    "            fatia.loc[fatia['unique_id'] == row['unique_id'], 'sentido'] = sentido\n",
    "            ponto = (row['latitude'], row['longitude'])\n",
    "            list_temp.append(row['unique_id'])\n",
    "            if margem_erro(ponto, pontos_finais[row['linha']][0]) and pontos_finais[row['linha']][0] != ultimo_final:\n",
    "                if garagem_ultimo == 1:\n",
    "                    garagem_ultimo = 0\n",
    "                    to_delete.append(list_temp[:])\n",
    "                sentido = abs(sentido -1)\n",
    "                list_temp[:] = []\n",
    "                ultimo_final = pontos_finais[row['linha']][0]\n",
    "            elif margem_erro(ponto, pontos_finais[row['linha']][1]) and pontos_finais[row['linha']][1] != ultimo_final:\n",
    "                if garagem_ultimo == 1:\n",
    "                    garagem_ultimo = 0\n",
    "                    to_delete.append(list_temp[:])\n",
    "                sentido = abs(sentido -1)\n",
    "                list_temp = []\n",
    "                ultimo_final = pontos_finais[row['linha']][1]\n",
    "            elif margem_erro(ponto, pontos_garagem[row['linha']], radius= 200):\n",
    "                garagem_ultimo = 1\n",
    "            if index == filtrados.index[-1] and garagem_ultimo == 1:\n",
    "                to_delete.append(list_temp[:])\n",
    "    to_delete_flat = [item for sublist in to_delete for item in sublist]\n",
    "    fatia = fatia[~fatia['unique_id'].isin(to_delete_flat)]\n",
    "    print(fatia['sentido'].value_counts())\n",
    "    fatia.to_sql('new_combined_table', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "483,639,309 759, 2803, 415, 292, 554, 422 pra caramba, 399, 2336, 638 pra cacete, 917, 803, 550, 457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo2 = 'dicionario_paradas.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(arquivo2, 'w') as f:\n",
    "    json.dump({'dicionario1': dicionario_paradas}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando tabelas para os dias de semana e fins de semana, dadas as diferenças de trajeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_da_semana = ['dia_2504_filter', 'dia_2604_filter', 'dia_2904_filter', 'dia_3004_filter', 'dia_0105_filter', 'dia_0205_filter', 'dia_0305_filter', 'dia_0605_filter', 'dia_0705_filter', 'dia_0805_filter', 'dia_0905_filter', 'dia_1005_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fins_de_semana = ['dia_2704_filter', 'dia_2804_filter', 'dia_0405_filter', 'dia_0505_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_dia_da_semana(nome_tabela):\n",
    "    data_str = nome_tabela.split('_')[1]\n",
    "    data = pd.to_datetime(f\"2024-{data_str[2:]}-{data_str[:2]}\")\n",
    "    return data.day_name()\n",
    "engine = create_engine('postgresql+psycopg2://savio:190876@localhost:5432/t3')\n",
    "for tabela in dias_da_semana:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {tabela}\", conn)\n",
    "    df['dia_da_semana'] = obter_dia_da_semana(tabela)\n",
    "    df.to_sql('dias_uteis_combinado', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tabela in fins_de_semana:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {tabela}\", conn)\n",
    "    df['dia_da_semana'] = obter_dia_da_semana(tabela)\n",
    "    df.to_sql('fds_combinado', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando uma tabela composta apenas pela madrugada dos fins de semana, onde a chance dos ônibus estarem na garagem é altíssima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def madrugada_fds(table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # Adicionar a coluna hour, se não existir\n",
    "        cur.execute(f\"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS hour INTEGER;\")\n",
    "        \n",
    "        # Atualizar a coluna hour com base no datahora\n",
    "        cur.execute(f\"UPDATE {table_name} SET hour = EXTRACT(HOUR FROM TO_TIMESTAMP(datahora / 1000));\")\n",
    "        \n",
    "        # Criar uma nova tabela filtrada\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE TABLE {table_name}_madrugada AS\n",
    "            SELECT *\n",
    "            FROM {table_name}\n",
    "            WHERE hour BETWEEN 22 AND 06;\n",
    "        \"\"\")\n",
    "        \n",
    "        # Remover a coluna hour das tabelas\n",
    "        cur.execute(f\"ALTER TABLE {table_name} DROP COLUMN hour;\")\n",
    "        cur.execute(f\"ALTER TABLE {table_name}_madrugada  DROP COLUMN hour;\")\n",
    "        \n",
    "        # Commit das alterações\n",
    "        conn.commit()\n",
    "        print(f\"Alterações na tabela {table_name} foram comitadas com sucesso.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Erro ao limpar dados da tabela {table_name}: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        print(f\"Cursor fechado para a tabela {table_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fins_de_semana_bruto = ['dia_2704', 'dia_2804', 'dia_0405', 'dia_0505']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alterações na tabela dia_2704 foram comitadas com sucesso.\n",
      "Cursor fechado para a tabela dia_2704.\n",
      "Alterações na tabela dia_2804 foram comitadas com sucesso.\n",
      "Cursor fechado para a tabela dia_2804.\n",
      "Alterações na tabela dia_0405 foram comitadas com sucesso.\n",
      "Cursor fechado para a tabela dia_0405.\n",
      "Alterações na tabela dia_0505 foram comitadas com sucesso.\n",
      "Cursor fechado para a tabela dia_0505.\n"
     ]
    }
   ],
   "source": [
    "for tabela in fins_de_semana_bruto:\n",
    "    madrugada_fds(tabela, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tabela in fins_de_semana_bruto:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {tabela}_madrugada\", conn)\n",
    "    df.to_sql('fds_noite', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentando encontrar as garagens a partir dos fins de semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14735/1545976685.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  teste = pd.read_sql(f\"SELECT * FROM dias_uteis_combinado WHERE linha = '315'\", conn)\n"
     ]
    }
   ],
   "source": [
    "teste = pd.read_sql(f\"SELECT * FROM dias_uteis_combinado WHERE linha = '315'\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'ordem', 'latitude', 'longitude', 'datahora', 'velocidade',\n",
       "       'linha', 'datahoraenvio', 'datahoraservidor', 'geom', 'dia_da_semana'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_grid(min_lat, min_lon, max_lat, max_lon, cell_size):\n",
    "    \"\"\"\n",
    "    Cria uma grid de células quadradas sobre uma área específica definida pelos limites de latitude e longitude.\n",
    "\n",
    "    Parameters:\n",
    "    min_lat (float): Latitude mínima da área.\n",
    "    min_lon (float): Longitude mínima da área.\n",
    "    max_lat (float): Latitude máxima da área.\n",
    "    max_lon (float): Longitude máxima da área.\n",
    "    cell_size (float): Tamanho da célula da grid em metros.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: Grid criada sobre a área especificada.\n",
    "    \"\"\"\n",
    "    # Inicializar lista de polígonos da grid\n",
    "    grid_polygons = []\n",
    "\n",
    "    # Latitude e longitude inicial\n",
    "    lat = min_lat\n",
    "    while lat < max_lat:\n",
    "        lon = min_lon\n",
    "        while lon < max_lon:\n",
    "            # Calcular o próximo ponto à direita e acima\n",
    "            d_lat = distance(meters=cell_size).destination(Point(lat, lon), 0).latitude - lat\n",
    "            d_lon = distance(meters=cell_size).destination(Point(lat, lon), 90).longitude - lon\n",
    "            \n",
    "            # Criar um polígono para a célula da grid\n",
    "            polygon = Polygon([\n",
    "                (lon, lat),\n",
    "                (lon + d_lon, lat),\n",
    "                (lon + d_lon, lat + d_lat),\n",
    "                (lon, lat + d_lat)\n",
    "            ])\n",
    "            \n",
    "            grid_polygons.append(polygon)\n",
    "            \n",
    "            lon += d_lon\n",
    "        lat += d_lat\n",
    "\n",
    "    # Criar um GeoDataFrame a partir dos polígonos da grid\n",
    "    grid = gpd.GeoDataFrame({'geometry': grid_polygons})\n",
    "    \n",
    "    # Definir o sistema de coordenadas EPSG:4326\n",
    "    grid.set_crs(epsg=4326, inplace=True)\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
